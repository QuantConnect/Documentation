<p>You need to <a href="/docs/v2/research-environment/machine-learning/tensorflow#04-Prepare-Data">prepare the historical data</a> for training before you train the model. If you have prepared the data, build and train the model. In this example, build a neural network model that predicts the future price of the SPY.</p>

<h4>Build the Model</h4>
<p>Follow these steps to build the model:</p>
<ol>
    <li>Set the number of layers, their number of nodes, the number of epoch and the learning rate.</li>
    <div class="section-example-container">
        <pre class="python">num_factors = X_test.shape[1]
num_neurons_1 = 10
num_neurons_2 = 20
num_neurons_3 = 5
epochs = 20
learning_rate = 0.0001</pre>
    </div>

    <li>Create hidden layers with the set number of layer and their corresponding number of nodes.</li>
    <p>In this example, we're constructing the model with the in-built Keras API, with Relu activator for non-linear activation of each tensors.</p>
    <div class="section-example-container">
        <pre class="python">model = tf.keras.Sequential([
    tf.keras.layers.Dense(num_neurons_1, activation=tf.nn.relu, input_shape=(num_factors,)),  # input shape required
    tf.keras.layers.Dense(num_neurons_2, activation=tf.nn.relu),
    tf.keras.layers.Dense(num_neurons_3, activation=tf.nn.relu),
    tf.keras.layers.Dense(1)
])</pre>
    </div>

    <li>Select an optimizer.</li>
    <p>We're using Adam optimizer in this example. You may also consider others like SGD.</p>
    <div class="section-example-container">
        <pre class="python">optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)</pre>
    </div>
    
    <li>Define the loss function.</li>
    <p>In the context of numerical regression, we use MSE as our objective function. If you're doing classification, cross entropy would be more suitable.</p>
    <div class="section-example-container">
        <pre class="python">def loss_mse(target_y, predicted_y):
    return tf.reduce_mean(tf.square(target_y - predicted_y))</pre>
    </div>
</ol>

<h4>Train the Model</h4>

<p>Iteratively train the model by the set epoch number. The model will train adaptively by the gradient provided by the loss function with the selected optimizer.</p>
<div class="section-example-container">
    <pre class="python">for i in range(epochs):
    with tf.GradientTape() as t:
        loss = loss_mse(y_train, model(X_train))

    train_loss = loss_mse(y_train, model(X_train))
    test_loss = loss_mse(y_test, model(X_test))
    print(f"""Epoch {i+1}:
Training loss = {train_loss.numpy()}. Test loss = {test_loss.numpy()}""")

    jac = t.gradient(loss, model.trainable_weights)
    optimizer.apply_gradients(zip(jac, model.trainable_weights))</pre>
</div>