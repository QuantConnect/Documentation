<p>The following examples demonstrate some common practices for using the <code>GPLearn</code> library.</p>

<h4>Example 1: Predict Return Direction</h4>
<p>The following research notebook uses <code>GPLearn</code> machine learning model to predict the next day's return direction by the previous 5 days' daily returns.</p>
<div class="section-example-container">
    <pre class="python"># Import the GPLearn library and others.
from gplearn.genetic import SymbolicRegressor, SymbolicTransformer
from sklearn.model_selection import train_test_split
import joblib

# Instantiate the QuantBook for researching.
qb = QuantBook()
# Request the daily SPY history with the date range to be studied.
symbol = qb.add_equity("SPY", Resolution.DAILY).symbol
history = qb.history(symbol, datetime(2020, 1, 1), datetime(2022, 1, 1)).loc[symbol]

# Obtain the daily returns to be the features and labels.
daily_returns = history['close'].pct_change()[1:]
# We use the previous 5 day returns as the features to be studied.
# Get the 1-day forward return direction as the labels for the machine to learn.
n_steps = 5
features = []
labels = []
for i in range(len(daily_returns)-n_steps):
    features.append(daily_returns.iloc[i:i+n_steps].values)
    labels.append(daily_returns.iloc[i+n_steps])

# Split the data as a training set and test set for validation.
X = np.array(features)
y = np.array(labels)
X_train, X_test, y_train, y_test = train_test_split(X, y)

# Declare a set of functions to use for feature engineering.
function_set = ['add', 'sub', 'mul', 'div',
                'sqrt', 'log', 'abs', 'neg', 'inv',
                'max', 'min']
# Call the SymbolicTransformer constructor with the preceding set of functions.
gp_transformer = SymbolicTransformer(function_set=function_set,
                                     random_state=0, 
                                     verbose=1)
# Call the fit method with the training features and labels to obtain the set of significant features.
gp_transformer.fit(X_train, y_train)
# Call the transform method to transform the original features.
gp_features_train = gp_transformer.transform(X_train)
# Call the hstack method with the original features and the transformed features to stack them.
new_X_train = np.hstack((X_train, gp_features_train))

# Call the SymbolicRegressor constructor for the non-linear regression fitting.
gp_regressor = SymbolicRegressor(random_state=0, verbose=1)
# Call the fit method with the engineered features and the original labels to fit a non-linear model.
gp_regressor.fit(new_X_train, y_train)

# Feature engineer the testing set data to test with.
gp_features_test = gp_transformer.transform(X_test)
new_X_test = np.hstack((X_test, gp_features_test))
# Call the predict method with the engineered testing set data to get the prediction from the GPLearn model.
y_predict = gp_regressor.predict(new_X_test)

# Plot the actual and predicted labels of the testing period.
df = pd.DataFrame({'Real': y_test.flatten(), 'Predicted': y_predict.flatten()})
df.plot(title='Model Performance: predicted vs actual closing price', figsize=(15, 10))
plt.show()

# Calculate the R-square value to evaluate the model fitness.
r2 = gp_regressor.score(new_X_test, y_test)
print(f"The explained variance of the GP model: {r2*100:.2f}%")

# Predict the label of the testing set data.
y_hat = predict(np.array(X_test))

# Store the model in the object store to allow accessing the model in the next research session or in the algorithm for trading.
transformer_key = "transformer"
regressor_key = "regressor"
transformer_file = qb.object_store.get_file_path(transformer_key)
regressor_file = qb.object_store.get_file_path(regressor_key)
joblib.dump(gp_transformer, transformer_file)
joblib.dump(gp_regressor, regressor_file)</pre>
</div>