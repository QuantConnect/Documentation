<p>In this example, a neural network regression prediction model using the following features and labels will be trained:</p>

<table class="qc-table table">
    <thead>
        <tr>
            <th>Data Category</th>
            <th>Description</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>Features</td>
            <td>The last 5 closing prices.</td>
        </tr>
        <tr>
            <td>Labels</td>
            <td>The following day's closing price</td>
        </tr>
    </tbody>
</table>

<p>The following image shows the time difference between the features and labels:</p>
<img class="docs-image" src="https://cdn.quantconnect.com/i/tu/ml-keras-function.png">

<p>Follow these steps to create a method to train your model:</p>

<h4>History Call</h4>
<p>You need historical data to train the model. Call <code>History</code> with <code>Symbol</code>, timerule, and resolution to obtain historical data. In this example, 2 years of daily trade bar data is used.</p>
<div class="section-example-container">
    <pre class="python">self.History(self.spy, 252*2, Resolution.Daily)</pre>
</div>

<h4>Prepare Data</h4>
<p>Follow the below steps to create a method to prepare the data for training and prediction.</p>
<ol>
    <li>Create a method to process the data for the algorithm class, with input data and number of timestep as arguments.</li>
    <div class="section-example-container">
        <pre class="python">def ProcessData(self, history, n_steps=5):</pre>
    </div>

    <li>Perform fractional differencing on the historical data.</li>
    <div class="section-example-container">
        <pre class="python">    df = (history['close'] * 0.5 + history['close'].diff() * 0.5)[1:]</pre>
    </div>
    <p>Fractional differencing helps make the data stationary yet retains the variance information.</p>

    <li>Loop through the <code>df</code> DataFrame and collect the features and labels.</li>
    <div class="section-example-container">
        <pre class="python">    features = []
    labels = []
    for i in range(len(df)-n_steps):
        features.append(df.iloc[i:i+n_steps].values)
        labels.append(df.iloc[i+n_steps])</pre>
    </div>

    <li>Convert the lists of features and labels into <code>numpy</code> arrays.</li>
    <div class="section-example-container">
        <pre class="python">    features = np.array(features)
    labels = np.array(labels)</pre>
    </div>

    <li>Standardize the features and labels, then return them.</li>
    <div class="section-example-container">
        <pre class="python">    features = (features - features.mean()) / features.std()
    labels = (labels - labels.mean()) / labels.std()

    return features, labels</pre>
    </div>
</ol>

<h4>Build Model</h4>
<p>Follow the below steps to build the model.</p>
<ol>
    <li>Get processed training data.</li>
    <div class="section-example-container">
        <pre class="python">features, labels = self.ProcessData(history)</pre>
    </div>

    <li>Define a subclass of <code>nn.Module</code> to be the model.</li>
    <p>In this example, use the ReLU activation function for each layer.</p>
    <div class="section-example-container">
        <pre class="python">class NeuralNetwork(nn.Module):
    # Model Structure
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(5, 5),   # input size, output size of the layer
            nn.ReLU(),         # Relu non-linear transformation
            nn.Linear(5, 5),
            nn.ReLU(),  
            nn.Linear(5, 1),   # Output size = 1 for regression
        )
    
    # Feed-forward training/prediction
    def forward(self, x):
        x = torch.from_numpy(x).float()   # Convert to tensor in type float
        result = self.linear_relu_stack(x)
        return result</pre>
    </div>

    <li>Create an instance of the model and set its configuration to train on the GPU if it's available.</li>
    <div class="section-example-container">
        <pre class="python">device = 'cuda' if torch.cuda.is_available() else 'cpu'
self.model = NeuralNetwork().to(device)</pre>
    </div>

    <li>Set the loss and optimization functions.</li>
    <p>In this example, use the mean squared error as the loss function and stochastic gradient descent as the optimizer.</p>
    <div class="section-example-container">
        <pre class="python">loss_fn = nn.MSELoss()
learning_rate = 0.001
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)</pre>
    </div>
</ol>

<h4>Fit Model</h4>
<p>Follow the below steps to fit the model with the prepared data.</p>
<ol>
    <li>Create a for-loop to train for preset number of epoch.</li>
    <p>In this example, train the model through 5 epochs.</p>
    <div class="section-example-container">
        <pre class="python">epochs = 5
for t in range(epochs):</pre>
    </div>

    <li>Create a for-loop to fit the model per batch.</li>
    <p>Since we're using SGD, we'll be using the size of data as batch number.</p>
    <div class="section-example-container">
        <pre class="python">    for batch, (feature, label) in enumerate(zip(features, labels)):</pre>
    </div>

    <li>Compute prediction and loss.</li>
    <div class="section-example-container">
        <pre class="python">        pred = self.model(feature)
        real = torch.from_numpy(np.array(label).flatten()).float()
        loss = loss_fn(pred, real)</pre>
    </div>

    <li>Perform backpropagation.</li>
    <div class="section-example-container">
        <pre class="python">        optimizer.zero_grad()
        loss.backward()
        optimizer.step()</pre>
    </div>
</ol>