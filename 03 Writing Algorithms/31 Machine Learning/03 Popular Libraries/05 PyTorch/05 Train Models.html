<p>You can train the model at the beginning of your algorithm and you can periodically re-train it as the algorithm executes.</p>

<h4>Warm Up Training Data</h4>
<p>You need historical data to initially train the model at the start of your algorithm. To get the initial training data, in the <code class="csharp">Initialize</code><code class="python">initialize</code> method, make a <a href="/docs/v2/writing-algorithms/historical-data/history-requests">history request</a>.</p>
<div class="section-example-container">
    <pre class="python">training_length = 252*2
self.training_data = RollingWindow[float](training_length)
history = self.history[TradeBar](self.symbol, training_length, Resolution.DAILY)
for trade_bar in history:
    self.training_data.add(trade_bar.close)</pre>
</div>

<h4>Define a Training Method</h4>
<p>To train the model, define a method that fits the model with the training data.</p>
<div class="section-example-container">
    <pre class="python">def get_features_and_labels(self, n_steps=5):
    close_prices = list(self.training_data)[::-1]

    features = []
    labels = []
    for i in range(len(close_prices)-n_steps):
        features.append(close_prices[i:i+n_steps])
        labels.append(close_prices[i+n_steps])
    features = np.array(features)
    labels = np.array(labels)

    return features, labels

def my_training_method(self):
    features, labels = self.get_features_and_labels()

    # Set the loss and optimization functions
    # In this example, use the mean squared error as the loss function and stochastic gradient descent as the optimizer
    loss_fn = nn.MSELoss()
    learning_rate = 0.001
    optimizer = torch.optim.SGD(self.model.parameters(), lr=learning_rate)
    
    # Create a for-loop to train for preset number of epoch
    epochs = 5
    for t in range(epochs):
        # Create a for-loop to fit the model per batch
        for batch, (feature, label) in enumerate(zip(features, labels)):
            # Compute prediction and loss
            pred = self.model(feature)
            real = torch.from_numpy(np.array(label).flatten()).float()
            loss = loss_fn(pred, real)
        
            # Perform backpropagation
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()</pre>
</div>

<h4>Set Training Schedule</h4>
<p>To train the model at the beginning of your algorithm, in the <code class="csharp">Initialize</code><code class="python">initialize</code> method, call the <code class="csharp">Train</code><code class="python">train</code> method.</p>
<div class="section-example-container">
    <pre class="python">self.train(self.my_training_method)</pre>
</div>
<p>To periodically re-train the model as your algorithm executes, in the <code class="csharp">Initialize</code><code class="python">initialize</code> method, call the <code class="csharp">Train</code><code class="python">train</code> method as a <a href="/docs/v2/writing-algorithms/scheduled-events">Scheduled Event</a>.</p>
<div class="section-example-container">
    <pre class="python"># Train the model every Sunday at 8:00 AM
self.train(self.date_rules.every(DayOfWeek.SUNDAY), self.time_rules.at(8, 0), self.my_training_method)</pre>
</div>


<h4>Update Training Data</h4>
<p>To update the training data as the algorithm executes, in the <code class="csharp">OnData</code><code class="python">on_data</code> method, add the current <code>TradeBar</code> to the <code>RollingWindow</code> that holds the training data.</p>
<div class="section-example-container">
    <pre class="python">def on_data(self, slice: Slice) -> None:
    if self.symbol in slice.Bars:
        self.training_data.Add(slice.Bars[self.symbol].Close)</pre>
</div>